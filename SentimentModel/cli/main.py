# -*- coding: utf-8 -*-
"""
å‘½ä»¤è¡Œå·¥å…·

æä¾›æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€é¢„æµ‹ç­‰å‘½ä»¤
"""

import argparse
import logging
import sys

from ..config import get_settings, LABEL_NAMES_CN


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


def cmd_stats(args):
    """ç»Ÿè®¡ä¿¡æ¯å‘½ä»¤"""
    from ..database import SentimentContentRepo, SentimentCommentRepo

    print("\n" + "=" * 60)
    print("æƒ…æ„Ÿåˆ†æç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)

    # å†…å®¹ç»Ÿè®¡
    content_total = SentimentContentRepo.count_all()
    content_analyzed = SentimentContentRepo.count_analyzed()
    content_unanalyzed = SentimentContentRepo.count_unanalyzed()

    print(f"\nğŸ“ å†…å®¹ç»Ÿè®¡:")
    print(f"   æ€»æ•°:       {content_total}")
    print(f"   å·²åˆ†æ:     {content_analyzed}")
    print(f"   å¾…åˆ†æ:     {content_unanalyzed}")

    # è¯„è®ºç»Ÿè®¡
    comment_total = SentimentCommentRepo.count_all()
    comment_analyzed = SentimentCommentRepo.count_analyzed()
    comment_unanalyzed = SentimentCommentRepo.count_unanalyzed()

    print(f"\nğŸ’¬ è¯„è®ºç»Ÿè®¡:")
    print(f"   æ€»æ•°:       {comment_total}")
    print(f"   å·²åˆ†æ:     {comment_analyzed}")
    print(f"   å¾…åˆ†æ:     {comment_unanalyzed}")

    # æƒ…æ„Ÿåˆ†å¸ƒ
    if content_analyzed > 0:
        print(f"\nğŸ“Š å†…å®¹æƒ…æ„Ÿåˆ†å¸ƒ:")
        stats = SentimentContentRepo.get_sentiment_stats()
        for row in stats:
            print(f"   {row['platform']} - {row['sentiment']}: {row['count']} (å¹³å‡åˆ†: {row['avg_score']:.2f})")

    if comment_analyzed > 0:
        print(f"\nğŸ“Š è¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ:")
        stats = SentimentCommentRepo.get_sentiment_stats()
        for row in stats:
            print(f"   {row['platform']} - {row['sentiment']}: {row['count']} (å¹³å‡åˆ†: {row['avg_score']:.2f})")

    print("\n" + "=" * 60)


def cmd_train(args):
    """è®­ç»ƒå‘½ä»¤"""
    from transformers import AutoTokenizer
    from ..models import BertSentimentClassifier
    from ..training import SentimentTrainer
    from ..data import load_public_dataset, split_dataset, create_dataloader
    from ..config import get_settings

    settings = get_settings()

    print(f"\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"è½®æ•°: {args.epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"è¾“å‡ºç›®å½•: {args.output}")

    # åŠ è½½æ•°æ®é›†
    print(f"\nåŠ è½½æ•°æ®é›†...")
    texts, labels = load_public_dataset(args.dataset)
    print(f"å…± {len(texts)} æ¡æ•°æ®")

    # åˆ’åˆ†æ•°æ®é›†
    splits = split_dataset(texts, labels)
    print(f"è®­ç»ƒé›†: {len(splits['train'][0])}, éªŒè¯é›†: {len(splits['val'][0])}, æµ‹è¯•é›†: {len(splits['test'][0])}")

    # åŠ è½½åˆ†è¯å™¨
    print(f"\nåŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(settings.model.name)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_dataloader(
        splits["train"][0], splits["train"][1],
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=settings.model.max_length,
        shuffle=True
    )
    val_loader = create_dataloader(
        splits["val"][0], splits["val"][1],
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=settings.model.max_length,
        shuffle=False
    )

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    print(f"\nåˆ›å»ºæ¨¡å‹...")
    model = BertSentimentClassifier(
        model_name=settings.model.name,
        num_labels=settings.model.num_labels,
        dropout=settings.model.dropout
    )

    # æ›´æ–°è®­ç»ƒé…ç½®
    settings.training.epochs = args.epochs
    settings.training.batch_size = args.batch_size
    settings.training.output_dir = args.output

    trainer = SentimentTrainer(model=model)

    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    result = trainer.train(train_loader, val_loader, epochs=args.epochs)

    print(f"\nè®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³ F1: {result['best_f1']:.4f}")
    print(f"æ¨¡å‹ä¿å­˜åˆ°: {result['output_dir']}")


def cmd_evaluate(args):
    """è¯„ä¼°å‘½ä»¤"""
    from transformers import AutoTokenizer
    from ..models import BertSentimentClassifier
    from ..training import SentimentTrainer
    from ..data import load_public_dataset, split_dataset, create_dataloader
    from ..config import get_settings

    settings = get_settings()

    print(f"\nè¯„ä¼°æ¨¡å‹: {args.model}")

    # åŠ è½½æ¨¡å‹
    model = BertSentimentClassifier.load(args.model)
    tokenizer = AutoTokenizer.from_pretrained(model.model_name)

    # åŠ è½½æµ‹è¯•æ•°æ®
    if args.test_data:
        import pandas as pd
        df = pd.read_csv(args.test_data)
        texts = df["text"].tolist()
        labels = df["label"].tolist()
    else:
        # ä½¿ç”¨é»˜è®¤æ•°æ®é›†çš„æµ‹è¯•é›†
        texts, labels = load_public_dataset(args.dataset)
        splits = split_dataset(texts, labels)
        texts, labels = splits["test"]

    print(f"æµ‹è¯•æ•°æ®: {len(texts)} æ¡")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = create_dataloader(
        texts, labels,
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=settings.model.max_length,
        shuffle=False
    )

    # è¯„ä¼°
    trainer = SentimentTrainer(model=model)
    result = trainer.evaluate(test_loader)

    print(result.summary())


def cmd_analyze(args):
    """åˆ†ææ•°æ®åº“æ•°æ®å‘½ä»¤"""
    from ..inference import SentimentPredictor, analyze_content, analyze_comment

    print(f"\nå¼€å§‹æƒ…æ„Ÿåˆ†æ...")
    print(f"æ¨¡å‹: {args.model}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    if args.dry_run:
        print("[è¯•è¿è¡Œæ¨¡å¼]")

    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = SentimentPredictor(model_path=args.model)

    # åˆ†æå†…å®¹
    if args.type in ["all", "content"]:
        print(f"\nåˆ†æå†…å®¹...")
        stats = analyze_content(predictor, batch_size=args.batch_size, dry_run=args.dry_run)
        print(f"å†…å®¹åˆ†æå®Œæˆ: æ€»æ•° {stats['total']}, æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['error']}")

    # åˆ†æè¯„è®º
    if args.type in ["all", "comment"]:
        print(f"\nåˆ†æè¯„è®º...")
        stats = analyze_comment(predictor, batch_size=args.batch_size, dry_run=args.dry_run)
        print(f"è¯„è®ºåˆ†æå®Œæˆ: æ€»æ•° {stats['total']}, æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['error']}")

    print(f"\nåˆ†æå®Œæˆ!")


def cmd_predict(args):
    """é¢„æµ‹å•æ¡æ–‡æœ¬å‘½ä»¤"""
    from ..inference import SentimentPredictor

    predictor = SentimentPredictor(model_path=args.model)
    result = predictor.predict(args.text)

    print(f"\nè¾“å…¥æ–‡æœ¬: {args.text}")
    print(f"\né¢„æµ‹ç»“æœ:")
    print(f"  æƒ…æ„Ÿ: {result.label} ({LABEL_NAMES_CN[result.label_id]})")
    print(f"  åˆ†æ•°: {result.score:.4f}")
    print(f"  æ¦‚ç‡åˆ†å¸ƒ:")
    for label, prob in result.probs.items():
        cn_name = LABEL_NAMES_CN[["negative", "neutral", "positive"].index(label)]
        print(f"    {cn_name} ({label}): {prob:.4f}")


def cmd_download(args):
    """ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å‘½ä»¤"""
    from transformers import AutoModel, AutoTokenizer

    print(f"\nä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹: {args.model}")

    print("ä¸‹è½½æ¨¡å‹...")
    AutoModel.from_pretrained(args.model)

    print("ä¸‹è½½åˆ†è¯å™¨...")
    AutoTokenizer.from_pretrained(args.model)

    print(f"\nä¸‹è½½å®Œæˆ! æ¨¡å‹å·²ç¼“å­˜åˆ°æœ¬åœ°ã€‚")


def create_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
    parser = argparse.ArgumentParser(
        prog="SentimentModel",
        description="ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹"
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # stats å‘½ä»¤
    stats_parser = subparsers.add_parser("stats", help="æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")

    # train å‘½ä»¤
    train_parser = subparsers.add_parser("train", help="è®­ç»ƒæ¨¡å‹")
    train_parser.add_argument(
        "--dataset", "-d",
        default="weibo_senti_100k",
        help="è®­ç»ƒæ•°æ®é›† (é»˜è®¤: weibo_senti_100k)"
    )
    train_parser.add_argument(
        "--epochs", "-e",
        type=int,
        default=3,
        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 3)"
    )
    train_parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=32,
        help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 32)"
    )
    train_parser.add_argument(
        "--output", "-o",
        default="models",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: models)"
    )

    # evaluate å‘½ä»¤
    eval_parser = subparsers.add_parser("evaluate", help="è¯„ä¼°æ¨¡å‹")
    eval_parser.add_argument(
        "--model", "-m",
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    eval_parser.add_argument(
        "--test-data",
        help="æµ‹è¯•æ•°æ® CSV æ–‡ä»¶"
    )
    eval_parser.add_argument(
        "--dataset",
        default="weibo_senti_100k",
        help="ä½¿ç”¨çš„æ•°æ®é›†"
    )
    eval_parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=64,
        help="æ‰¹æ¬¡å¤§å°"
    )

    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser("analyze", help="åˆ†ææ•°æ®åº“ä¸­çš„æ•°æ®")
    analyze_parser.add_argument(
        "--model", "-m",
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    analyze_parser.add_argument(
        "--type", "-t",
        choices=["all", "content", "comment"],
        default="all",
        help="åˆ†æç±»å‹ (é»˜è®¤: all)"
    )
    analyze_parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=100,
        help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 100)"
    )
    analyze_parser.add_argument(
        "--dry-run",
        action="store_true",
        help="è¯•è¿è¡Œ (ä¸ä¿å­˜ç»“æœ)"
    )

    # predict å‘½ä»¤
    predict_parser = subparsers.add_parser("predict", help="é¢„æµ‹å•æ¡æ–‡æœ¬")
    predict_parser.add_argument(
        "--model", "-m",
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    predict_parser.add_argument(
        "--text", "-t",
        required=True,
        help="è¦é¢„æµ‹çš„æ–‡æœ¬"
    )

    # download å‘½ä»¤
    download_parser = subparsers.add_parser("download", help="ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")
    download_parser.add_argument(
        "--model", "-m",
        default="hfl/chinese-roberta-wwm-ext-base",
        help="æ¨¡å‹åç§° (é»˜è®¤: hfl/chinese-roberta-wwm-ext-base)"
    )

    return parser


# å‘½ä»¤æ˜ å°„
COMMANDS = {
    "stats": cmd_stats,
    "train": cmd_train,
    "evaluate": cmd_evaluate,
    "analyze": cmd_analyze,
    "predict": cmd_predict,
    "download": cmd_download,
}


def main():
    """ä¸»å…¥å£"""
    parser = create_parser()
    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        sys.exit(0)

    if args.command in COMMANDS:
        try:
            COMMANDS[args.command](args)
        except KeyboardInterrupt:
            print("\næ“ä½œå·²å–æ¶ˆ")
            sys.exit(1)
        except Exception as e:
            logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
            raise
    else:
        parser.print_help()
        sys.exit(1)


# ä¸ºäº†å…¼å®¹æ€§
app = main


if __name__ == "__main__":
    main()
