# -*- coding: utf-8 -*-
"""
æ•°æ®å‡†å¤‡æ¨¡å—

å°†å¾®åšæƒ…æ„Ÿæ•°æ®é›†è½¬æ¢ä¸º Qwen2.5 æŒ‡ä»¤å¾®è°ƒæ ¼å¼
"""

import json
import random
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


# æƒ…ç»ªæ ‡ç­¾å®šä¹‰ (å‚è€ƒ GoEmotions ä½†é’ˆå¯¹ä¸­æ–‡åœºæ™¯ä¼˜åŒ–)
EMOTION_TAGS = [
    "å–œæ‚¦", "å…´å¥‹", "æ»¡è¶³", "æ„Ÿæ¿€", "çˆ±",          # æ­£é¢æƒ…ç»ª
    "æ„¤æ€’", "åŒæ¶", "æ‚²ä¼¤", "ææƒ§", "å¤±æœ›",        # è´Ÿé¢æƒ…ç»ª
    "æƒŠè®¶", "å›°æƒ‘", "å¥½å¥‡", "æœŸå¾…", "ç„¦è™‘",        # å¤æ‚æƒ…ç»ª
    "å¹³é™", "æ— èŠ", "å†·æ¼ ",                        # ä¸­æ€§æƒ…ç»ª
]

# æƒ…ç»ªåˆ°æƒ…æ„Ÿçš„æ˜ å°„
EMOTION_TO_SENTIMENT = {
    "å–œæ‚¦": "positive", "å…´å¥‹": "positive", "æ»¡è¶³": "positive",
    "æ„Ÿæ¿€": "positive", "çˆ±": "positive",
    "æ„¤æ€’": "negative", "åŒæ¶": "negative", "æ‚²ä¼¤": "negative",
    "ææƒ§": "negative", "å¤±æœ›": "negative",
    "æƒŠè®¶": "neutral", "å›°æƒ‘": "neutral", "å¥½å¥‡": "neutral",
    "æœŸå¾…": "positive", "ç„¦è™‘": "negative",
    "å¹³é™": "neutral", "æ— èŠ": "neutral", "å†·æ¼ ": "neutral",
}

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š
1. sentiment: æƒ…æ„Ÿå€¾å‘ (positive/negative/neutral)
2. sentiment_score: æƒ…æ„Ÿåˆ†æ•° (-1.0 åˆ° 1.0ï¼Œè´Ÿé¢ä¸ºè´Ÿï¼Œæ­£é¢ä¸ºæ­£)
3. emotion_tags: æƒ…ç»ªæ ‡ç­¾åˆ—è¡¨

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºç»“æœã€‚"""


def create_instruction_prompt(text: str) -> str:
    """åˆ›å»ºæŒ‡ä»¤æç¤ºè¯"""
    return f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š\n\n{text}"


def create_response(
    sentiment: str,
    sentiment_score: float,
    emotion_tags: List[str]
) -> str:
    """åˆ›å»ºå“åº”å†…å®¹"""
    result = {
        "sentiment": sentiment,
        "sentiment_score": round(sentiment_score, 2),
        "emotion_tags": emotion_tags
    }
    return json.dumps(result, ensure_ascii=False, indent=2)


def infer_emotion_tags(text: str, sentiment: str) -> List[str]:
    """
    æ ¹æ®æ–‡æœ¬å†…å®¹æ¨æ–­æƒ…ç»ªæ ‡ç­¾

    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¯å‘å¼æ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è§„åˆ™æˆ–é¢„æ ‡æ³¨æ•°æ®
    """
    text_lower = text.lower()
    tags = []

    # æ­£é¢æƒ…ç»ªå…³é”®è¯
    positive_keywords = {
        "å–œæ‚¦": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å¤ªå¥½äº†", "å“ˆå“ˆ", "å˜»å˜»", "æ£’", "å¥½", "èµ"],
        "å…´å¥‹": ["æ¿€åŠ¨", "å…´å¥‹", "å¤ªæ£’äº†", "å¤ªå‰å®³", "ç‰›", "nb", "awesome"],
        "æ»¡è¶³": ["æ»¡è¶³", "æ»¡æ„", "èˆ’æœ", "äº«å—", "å¹¸ç¦"],
        "æ„Ÿæ¿€": ["æ„Ÿè°¢", "è°¢è°¢", "å¤šäº", "æ„Ÿæ©", "æ„Ÿæ¿€"],
        "çˆ±": ["çˆ±", "å–œæ¬¢", "çˆ±ä½ ", "å¿ƒ", "â¤", "ğŸ’•", "ğŸ˜"],
    }

    # è´Ÿé¢æƒ…ç»ªå…³é”®è¯
    negative_keywords = {
        "æ„¤æ€’": ["æ°”æ­»", "ç”Ÿæ°”", "æ„¤æ€’", "æ€’", "ç«å¤§", "å¯æ¶", "æ“", "è‰¹"],
        "åŒæ¶": ["æ¶å¿ƒ", "è®¨åŒ", "çƒ¦", "åƒåœ¾", "æ¶", "å‘¸"],
        "æ‚²ä¼¤": ["éš¾è¿‡", "ä¼¤å¿ƒ", "å“­", "æ‚²", "ç—›", "ğŸ˜¢", "ğŸ˜­"],
        "ææƒ§": ["å®³æ€•", "ææƒ§", "å“", "æ€•", "æ‹…å¿ƒ"],
        "å¤±æœ›": ["å¤±æœ›", "é—æ†¾", "å¯æƒœ", "å”‰", "å·®", "çƒ‚"],
    }

    # ä¸­æ€§/å¤æ‚æƒ…ç»ªå…³é”®è¯
    neutral_keywords = {
        "æƒŠè®¶": ["æƒŠ", "éœ‡æƒŠ", "æ²¡æƒ³åˆ°", "å±…ç„¶", "ç«Ÿç„¶", "wow"],
        "å›°æƒ‘": ["å›°æƒ‘", "è¿·æƒ‘", "ä¸æ‡‚", "ä»€ä¹ˆæ„æ€", "ä¸ºä»€ä¹ˆ"],
        "å¥½å¥‡": ["å¥½å¥‡", "æƒ³çŸ¥é“", "æ˜¯ä»€ä¹ˆ", "æ€ä¹ˆ"],
        "æœŸå¾…": ["æœŸå¾…", "å¸Œæœ›", "ç›¼", "ç­‰"],
        "ç„¦è™‘": ["ç„¦è™‘", "ç€æ€¥", "æ€¥", "æ…Œ"],
    }

    all_keywords = {**positive_keywords, **negative_keywords, **neutral_keywords}

    for emotion, keywords in all_keywords.items():
        for kw in keywords:
            if kw in text_lower:
                if emotion not in tags:
                    tags.append(emotion)
                break

    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å…·ä½“æƒ…ç»ªï¼Œæ ¹æ®æƒ…æ„Ÿå€¾å‘ç»™é»˜è®¤æ ‡ç­¾
    if not tags:
        if sentiment == "positive":
            tags = ["å–œæ‚¦"]
        elif sentiment == "negative":
            tags = ["å¤±æœ›"]
        else:
            tags = ["å¹³é™"]

    return tags[:3]  # æœ€å¤šè¿”å›3ä¸ªæ ‡ç­¾


def convert_weibo_label(label: int) -> Tuple[str, float]:
    """
    è½¬æ¢å¾®åšæ•°æ®é›†æ ‡ç­¾

    Args:
        label: åŸå§‹æ ‡ç­¾ (0=è´Ÿé¢, 1=æ­£é¢)

    Returns:
        (sentiment, sentiment_score) å…ƒç»„
    """
    if label == 0:
        return "negative", random.uniform(-0.9, -0.5)
    else:
        return "positive", random.uniform(0.5, 0.9)


def prepare_sentiment_data(
    input_file: Optional[str] = None,
    output_dir: str = "data/qwen_finetune",
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    max_samples: Optional[int] = None,
    seed: int = 42
) -> Dict[str, str]:
    """
    å‡†å¤‡å¾®è°ƒæ•°æ®é›†

    Args:
        input_file: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ (å¾®åšæ•°æ®é›†)
        output_dir: è¾“å‡ºç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        max_samples: æœ€å¤§æ ·æœ¬æ•° (ç”¨äºæµ‹è¯•)
        seed: éšæœºç§å­

    Returns:
        åŒ…å« train/val/test æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    import pandas as pd
    from datasets import load_dataset

    random.seed(seed)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    if input_file:
        df = pd.read_csv(input_file)
        texts = df["review"].tolist()
        labels = df["label"].tolist()
    else:
        # ä½¿ç”¨ HuggingFace æ•°æ®é›†
        logger.info("ä» HuggingFace åŠ è½½ weibo_senti_100k æ•°æ®é›†...")
        dataset = load_dataset("IDEA-CCNL/weibo_senti_100k", split="train")
        texts = dataset["review"]
        labels = dataset["label"]

    if max_samples:
        indices = random.sample(range(len(texts)), min(max_samples, len(texts)))
        texts = [texts[i] for i in indices]
        labels = [labels[i] for i in indices]

    logger.info(f"å…± {len(texts)} æ¡æ•°æ®")

    # è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼
    data = []
    for text, label in zip(texts, labels):
        sentiment, score = convert_weibo_label(label)
        emotion_tags = infer_emotion_tags(text, sentiment)

        instruction = create_instruction_prompt(text)
        response = create_response(sentiment, score, emotion_tags)

        data.append({
            "instruction": instruction,
            "input": "",
            "output": response,
            "system": SYSTEM_PROMPT,
            # ä¿ç•™åŸå§‹ä¿¡æ¯ç”¨äºéªŒè¯
            "_text": text,
            "_label": label,
        })

    # æ‰“ä¹±æ•°æ®
    random.shuffle(data)

    # åˆ’åˆ†æ•°æ®é›†
    n = len(data)
    n_train = int(n * train_ratio)
    n_val = int(n * val_ratio)

    train_data = data[:n_train]
    val_data = data[n_train:n_train + n_val]
    test_data = data[n_train + n_val:]

    logger.info(f"è®­ç»ƒé›†: {len(train_data)}, éªŒè¯é›†: {len(val_data)}, æµ‹è¯•é›†: {len(test_data)}")

    # ä¿å­˜æ•°æ®
    paths = {}
    for split, split_data in [("train", train_data), ("val", val_data), ("test", test_data)]:
        file_path = output_path / f"{split}.json"
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(split_data, f, ensure_ascii=False, indent=2)
        paths[split] = str(file_path)
        logger.info(f"ä¿å­˜ {split} åˆ° {file_path}")

    return paths


def load_finetune_data(file_path: str) -> List[Dict]:
    """åŠ è½½å¾®è°ƒæ•°æ®"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æµ‹è¯•æ•°æ®å‡†å¤‡
    paths = prepare_sentiment_data(
        output_dir="data/qwen_finetune",
        max_samples=1000  # æµ‹è¯•ç”¨
    )
    print(f"æ•°æ®å·²å‡†å¤‡: {paths}")
