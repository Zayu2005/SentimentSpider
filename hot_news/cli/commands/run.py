# =====================================================
# Hot News Module - Run Command (ä¸€é”®æ‰§è¡Œ)
# =====================================================

import typer
from typing import List, Optional
import asyncio
from datetime import datetime
import time
from hot_news.config import get_settings
from hot_news.fetcher import HotNewsFactory
from hot_news.analyzer import DomainChecker, KeywordExtractor
from hot_news.crawler import CrawlTrigger
from hot_news.database import (
    HotNewsRepository,
    AnalysisRepository,
    KeywordRepository,
    TaskLogRepository,
)
from hot_news.models.entities import HotNewsItem

# å¹¶å‘æ§åˆ¶ï¼šæœ€å¤šåŒæ—¶æ‰§è¡Œçš„LLMè°ƒç”¨æ•°
MAX_CONCURRENT_LLM_CALLS = 5

cmd_run = typer.Typer(help="ä¸€é”®æ‰§è¡Œå®Œæ•´æµç¨‹")


async def _analyze_domain(news_list, selected_domains):
    """å¼‚æ­¥åˆ†æé¢†åŸŸåŒ¹é… - ä½¿ç”¨å¹¶å‘åŠ é€Ÿ"""
    checker = DomainChecker()
    analysis_repo = AnalysisRepository()
    matched_count = 0

    # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)

    async def check_and_save(news_row, domain):
        """æ£€æŸ¥å•ä¸ªæ–°é—»-é¢†åŸŸå¯¹ï¼Œå¹¶ä¿å­˜ç»“æœ"""
        async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
            news = HotNewsItem(
                news_id=news_row["news_id"],
                platform_code=news_row["platform_code"],
                title=news_row["title"],
                url=news_row.get("url", ""),
                description=news_row.get("description", ""),
            )

            try:
                result = await checker.check_domain(news, domain)
                analysis_repo.save_analysis(
                    news_id=result.news_id,
                    domain_id=result.domain_id,
                    is_match=result.is_match,
                    llm_provider="",
                    analysis_content=result.reason,
                    confidence=result.confidence,
                )
                return 1 if result.is_match else 0
            except Exception as e:
                print(f"  [é”™è¯¯] {news_row['title'][:20]}... @ {domain.domain_name}: {str(e)[:40]}")
                return 0

    # åˆ›å»ºæ‰€æœ‰çš„æ£€æŸ¥ä»»åŠ¡
    tasks = []
    for news_row in news_list:
        for domain in selected_domains:
            task = check_and_save(news_row, domain)
            tasks.append(task)

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ç»Ÿè®¡åŒ¹é…æ•°
    for result in results:
        if isinstance(result, int):
            matched_count += result

    return matched_count


async def _extract_keywords(selected_domains, keyword_limit, all_news=None, run_batch_id: int = None):
    """å¼‚æ­¥æå–å…³é”®è¯ - ä½¿ç”¨å¹¶å‘åŠ é€Ÿ

    Args:
        selected_domains: é€‰å®šçš„é¢†åŸŸåˆ—è¡¨ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨all_news
        keyword_limit: æ¯ä¸ªé¢†åŸŸæœ€å¤šæå–çš„å…³é”®è¯æ•°
        all_news: å½“selected_domainsä¸ºç©ºæ—¶ï¼Œä»è¿™ä¸ªæ–°é—»åˆ—è¡¨ä¸­æå–å…³é”®è¯
        run_batch_id: è¿è¡Œæ‰¹æ¬¡IDï¼Œç”¨äºè¿½è¸ªè¯¥æ‰¹æ¬¡æå–çš„å…³é”®è¯
    """
    extractor = KeywordExtractor()
    keyword_repo = KeywordRepository()
    total_keywords = 0

    # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_LLM_CALLS)

    async def extract_and_save(news_row, domain):
        """æå–å…³é”®è¯å¹¶ä¿å­˜"""
        async with semaphore:
            news = HotNewsItem(
                news_id=news_row["news_id"],
                platform_code=news_row["platform_code"],
                title=news_row["title"],
                url=news_row.get("url", ""),
                description=news_row.get("description", ""),
            )

            try:
                keywords = await extractor.extract_keywords(news, domain)
                if keywords:
                    keyword_repo.bulk_save(keywords, run_batch_id=run_batch_id)
                    return len(keywords)
            except Exception as e:
                print(f"  [é”™è¯¯] æå–å…³é”®è¯å¤±è´¥ {news_row['title'][:20]}...: {str(e)[:40]}")
            return 0

    tasks = []

    # å¦‚æœæœ‰domainé…ç½®ï¼Œä»åŒ¹é…çš„çƒ­ç‚¹ä¸­æå–
    if selected_domains:
        for domain in selected_domains:
            matched_news = AnalysisRepository().get_matched_news(domain.id, keyword_limit)
            for news_row in matched_news:
                task = extract_and_save(news_row, domain)
                tasks.append(task)

    # å¦‚æœæ²¡æœ‰domainé…ç½®ï¼Œä»æ‰€æœ‰çƒ­ç‚¹ä¸­æå–ï¼ˆä¸ç­›é€‰ï¼‰
    elif all_news:
        from hot_news.models.entities import DomainConfig
        virtual_domain = DomainConfig(
            id=0,
            domain_name="é€šç”¨",
            domain_keywords="çƒ­ç‚¹,æ–°é—»",
            is_enabled=1,
        )

        for news_row in all_news[:keyword_limit]:
            task = extract_and_save(news_row, virtual_domain)
            tasks.append(task)

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    if tasks:
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result in results:
            if isinstance(result, int):
                total_keywords += result

    return total_keywords


async def _run_pipeline_inner(
    platforms, domains, crawl_platforms, hot_limit, keyword_limit, no_llm, no_crawl
):
    """å†…éƒ¨å¼‚æ­¥æ‰§è¡Œå‡½æ•°"""
    start_time = time.time()
    start_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    settings = get_settings()
    log_repo = TaskLogRepository()

    log_id = log_repo.start_task("hot_pipeline")

    hot_count = 0
    matched_count = 0
    keyword_count = 0
    crawl_count = 0

    try:
        # =============== æ˜¾ç¤ºæ‰§è¡Œé…ç½® ===============
        print("\n" + "=" * 70)
        print("  ğŸš€ çƒ­ç‚¹æ–°é—»è·å–ä¸åˆ†ææµç¨‹")
        print("=" * 70)
        print(f"â° å¼€å§‹æ—¶é—´: {start_datetime}")
        print(f"ğŸ“‹ æ‰¹æ¬¡ID: {log_id}")
        print()

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print("ğŸ“ æ‰§è¡Œé…ç½®:")
        print(f"  â€¢ çƒ­ç‚¹é™åˆ¶: {hot_limit} æ¡/å¹³å°")
        print(f"  â€¢ å…³é”®è¯é™åˆ¶: {keyword_limit} ä¸ª/é¢†åŸŸ")
        print(f"  â€¢ LLMåˆ†æ: {'âœ“ å¯ç”¨' if not no_llm else 'âœ— ç¦ç”¨'}")
        print(f"  â€¢ çˆ¬è™«è§¦å‘: {'âœ“ å¯ç”¨' if not no_crawl else 'âœ— ç¦ç”¨'}")

        if platforms:
            print(f"  â€¢ çƒ­ç‚¹å¹³å°: {', '.join(platforms)}")

        if domains:
            print(f"  â€¢ åˆ†æé¢†åŸŸ: {', '.join(domains)}")

        if crawl_platforms:
            print(f"  â€¢ çˆ¬è™«å¹³å°: {', '.join(crawl_platforms)}")
        print()

        # =============== Step 1: è·å–çƒ­ç‚¹ ===============
        print("[Step 1/4] ğŸ” è·å–çƒ­ç‚¹æ–°é—»")
        print("-" * 70)

        if platforms:
            selected_platforms = platforms
        else:
            selected_platforms = [p.platform_code for p in settings.get_hot_platforms()]

        repo = HotNewsRepository()

        print(f"ä» {len(selected_platforms)} ä¸ªå¹³å°è·å–çƒ­ç‚¹...")

        total = 0
        platform_results = []
        for platform in selected_platforms:
            try:
                fetcher = HotNewsFactory.get_fetcher(platform)
                news_list = await fetcher.fetch(hot_limit)
                if news_list:
                    repo.bulk_save(news_list)
                    total += len(news_list)
                    platform_results.append((platform, len(news_list), "âœ“"))
                    print(f"  âœ“ {platform:15} {len(news_list):3} æ¡")
                else:
                    platform_results.append((platform, 0, "âœ—"))
                    print(f"  - {platform:15}  æ— æ•°æ®")
            except Exception as e:
                platform_results.append((platform, 0, "âœ—"))
                print(f"  âœ— {platform:15} é”™è¯¯: {str(e)[:50]}")

        await HotNewsFactory.close_all()
        hot_count = total
        print(f"\nâœ… æ­¥éª¤å®Œæˆ: å…±è·å– {hot_count} æ¡çƒ­ç‚¹")

        # =============== Step 2: é¢†åŸŸåˆ†æ ===============
        print("\n[Step 2/4] ğŸ¯ åˆ†æé¢†åŸŸåŒ¹é…")
        print("-" * 70)

        domain_configs = settings.get_domains()
        if domains:
            selected_domains = [d for d in domain_configs if d.domain_name in domains]
        else:
            selected_domains = domain_configs

        if selected_domains and not no_llm:
            print(f"ä½¿ç”¨ {len(selected_domains)} ä¸ªé¢†åŸŸè¿›è¡Œåˆ†æ...")
            for domain in selected_domains:
                print(f"  â€¢ {domain.domain_name} (ID:{domain.id})")

            news_list = repo.get_recent(hot_limit)
            print(f"\nä» {len(news_list)} æ¡çƒ­ç‚¹ä¸­è¿›è¡Œåˆ†æ...")
            print(f"âš¡ å¹¶å‘æ‰§è¡Œ (æœ€å¤š {MAX_CONCURRENT_LLM_CALLS} å¹¶å‘)")
            matched_count = await _analyze_domain(news_list, selected_domains)
            print(f"\nâœ… æ­¥éª¤å®Œæˆ: åŒ¹é… {matched_count} æ¡çƒ­ç‚¹")
        elif not selected_domains and not no_llm:
            print("â­ï¸  è·³è¿‡åˆ†æ (æœªé…ç½®é¢†åŸŸ)")
            print("âœ… æ­¥éª¤å®Œæˆ: è·³è¿‡")
        else:
            print("â­ï¸  è·³è¿‡åˆ†æ (--no-llm æ ‡å¿—)")
            print("âœ… æ­¥éª¤å®Œæˆ: è·³è¿‡")

        # =============== Step 3: æå–å…³é”®è¯ ===============
        print("\n[Step 3/4] ğŸ”‘ æå–å…³é”®è¯")
        print("-" * 70)

        if not no_llm:
            if selected_domains:
                print(f"ä» {len(selected_domains)} ä¸ªé¢†åŸŸçš„åŒ¹é…çƒ­ç‚¹ä¸­æå–å…³é”®è¯...")
                print(f"âš¡ å¹¶å‘æ‰§è¡Œ (æœ€å¤š {MAX_CONCURRENT_LLM_CALLS} å¹¶å‘)")
                keyword_count = await _extract_keywords(selected_domains, keyword_limit, run_batch_id=log_id)
                print(f"\nâœ… æ­¥éª¤å®Œæˆ: æå– {keyword_count} ä¸ªå…³é”®è¯")
            else:
                print("æœªé…ç½®é¢†åŸŸï¼Œä»æ‰€æœ‰çƒ­ç‚¹ä¸­æå–å…³é”®è¯...")
                print(f"âš¡ å¹¶å‘æ‰§è¡Œ (æœ€å¤š {MAX_CONCURRENT_LLM_CALLS} å¹¶å‘)")
                all_news_list = repo.get_recent(hot_limit)
                keyword_count = await _extract_keywords([], keyword_limit, all_news=all_news_list, run_batch_id=log_id)
                print(f"\nâœ… æ­¥éª¤å®Œæˆ: æå– {keyword_count} ä¸ªå…³é”®è¯")
        else:
            print("â­ï¸  è·³è¿‡æå– (--no-llm æ ‡å¿—)")
            print("âœ… æ­¥éª¤å®Œæˆ: è·³è¿‡")

        # =============== Step 4: è§¦å‘çˆ¬è™« ===============
        print("\n[Step 4/4] ğŸ•·ï¸  è§¦å‘çˆ¬è™«")
        print("-" * 70)

        if not no_crawl:
            trigger = CrawlTrigger()
            keyword_repo = KeywordRepository()

            if crawl_platforms:
                crawl_platform_list = crawl_platforms
            else:
                crawl_platform_list = [
                    p.platform_code for p in settings.get_crawler_platforms()
                ]

            print(f"ç›®æ ‡çˆ¬è™«å¹³å°: {', '.join(crawl_platform_list)}")

            # ä½¿ç”¨run_batch_idè¿‡æ»¤ï¼Œåªçˆ¬å–å½“å‰è¿è¡Œæ‰¹æ¬¡çš„å…³é”®è¯
            keywords = keyword_repo.get_by_batch_never_crawled(run_batch_id=log_id, limit=20)

            if keywords:
                print(f"å‡†å¤‡çˆ¬å– {len(keywords)} ä¸ªå…³é”®è¯...")
                success_count = 0
                fail_count = 0

                for kw in keywords:
                    for platform in crawl_platform_list:
                        try:
                            if trigger.trigger_crawl(kw["keyword"], platform, 30, 10):
                                keyword_repo.increment_search_count(kw["id"])
                                crawl_count += 1
                                success_count += 1
                                print(f"  âœ“ {kw['keyword']:20} @ {platform}")
                            else:
                                fail_count += 1
                                print(f"  âœ— {kw['keyword']:20} @ {platform} (å¤±è´¥)")
                        except Exception as e:
                            fail_count += 1
                            print(f"  âœ— {kw['keyword']:20} @ {platform} (é”™è¯¯: {str(e)[:30]})")

                print(f"\nâœ… æ­¥éª¤å®Œæˆ: æˆåŠŸè§¦å‘ {success_count} æ¬¡, å¤±è´¥ {fail_count} æ¬¡")
            else:
                print("â­ï¸  æ— å¾…çˆ¬å–å…³é”®è¯ (å½“å‰æ‰¹æ¬¡æ— æå–çš„å…³é”®è¯)")
                print("âœ… æ­¥éª¤å®Œæˆ: è·³è¿‡")
        else:
            print("â­ï¸  è·³è¿‡çˆ¬è™« (--no-crawl æ ‡å¿—)")
            print("âœ… æ­¥éª¤å®Œæˆ: è·³è¿‡")

        # =============== æ‰§è¡Œæ€»ç»“ ===============
        end_time = time.time()
        elapsed_time = end_time - start_time
        end_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        print("\n" + "=" * 70)
        print("  âœ… æ‰§è¡Œå®Œæˆ!")
        print("=" * 70)
        print(f"ğŸ“Š æ‰§è¡Œç»“æœ:")
        print(f"  â€¢ è·å–çƒ­ç‚¹: {hot_count:4} æ¡")
        print(f"  â€¢ åŒ¹é…çƒ­ç‚¹: {matched_count:4} æ¡")
        print(f"  â€¢ æå–å…³é”®è¯: {keyword_count:3} ä¸ª")
        print(f"  â€¢ è§¦å‘çˆ¬è™«: {crawl_count:3} æ¬¡")
        print()
        print(f"â±ï¸  æ‰§è¡Œè€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ”š ç»“æŸæ—¶é—´: {end_datetime}")
        print("=" * 70)

        log_repo.complete_task(
            log_id, "success", hot_count, matched_count, keyword_count, crawl_count
        )

    except Exception as e:
        end_time = time.time()
        elapsed_time = end_time - start_time

        print("\n" + "=" * 70)
        print("  âŒ æ‰§è¡Œå¤±è´¥!")
        print("=" * 70)
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"â±ï¸  æ‰§è¡Œè€—æ—¶: {elapsed_time:.2f} ç§’")
        print("=" * 70)

        log_repo.complete_task(
            log_id,
            "failed",
            hot_count,
            matched_count,
            keyword_count,
            crawl_count,
            str(e),
        )
        raise


@cmd_run.command()
def run_pipeline(
    platforms: Optional[List[str]] = typer.Argument(None, help="çƒ­ç‚¹å¹³å°åˆ—è¡¨"),
    domains: Optional[List[str]] = typer.Option(
        None, "--domains", "-d", help="é¢†åŸŸåˆ—è¡¨"
    ),
    crawl_platforms: Optional[List[str]] = typer.Option(
        None, "--crawl-platforms", "-cp", help="çˆ¬è™«å¹³å°"
    ),
    hot_limit: int = typer.Option(50, "--hot-limit", "-hl", help="è·å–çƒ­ç‚¹æ•°é‡"),
    keyword_limit: int = typer.Option(
        20, "--keyword-limit", "-kl", help="æå–å…³é”®è¯æ•°é‡"
    ),
    no_llm: bool = typer.Option(False, "--no-llm", help="è·³è¿‡LLMåˆ†æï¼Œç›´æ¥æå–å…³é”®è¯"),
    no_crawl: bool = typer.Option(False, "--no-crawl", help="è·³è¿‡çˆ¬è™«è§¦å‘"),
):
    """
    ä¸€é”®æ‰§è¡Œå®Œæ•´æµç¨‹ï¼šè·å–çƒ­ç‚¹ -> åˆ†æåŒ¹é… -> æå–å…³é”®è¯ -> è§¦å‘çˆ¬è™«

    ç¤ºä¾‹:
        hot-news run weibo zhihu ç§‘æŠ€ é‡‘è
        hot-news run --hot-limit 100 --crawl-platforms xhs dy
    """
    asyncio.run(
        _run_pipeline_inner(
            platforms,
            domains,
            crawl_platforms,
            hot_limit,
            keyword_limit,
            no_llm,
            no_crawl,
        )
    )
