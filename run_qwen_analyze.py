# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ Qwen2.5 åˆ†ææ•°æ®åº“ä¸­çš„æ•°æ®

ç”¨æ³•:
    python run_qwen_analyze.py                    # åˆ†æå…¨éƒ¨æ•°æ®
    python run_qwen_analyze.py --type content     # åªåˆ†æå†…å®¹
    python run_qwen_analyze.py --type comment     # åªåˆ†æè¯„è®º
    python run_qwen_analyze.py --batch-size 50    # è®¾ç½®æ‰¹æ¬¡å¤§å°
    python run_qwen_analyze.py --dry-run          # è¯•è¿è¡Œ (ä¸ä¿å­˜)
"""

import os
import re
import json
import argparse
import logging
from typing import List, Dict, Any

# è®¾ç½® HuggingFace é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


# æ¨¡å‹é…ç½®
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š
1. sentiment: æƒ…æ„Ÿå€¾å‘ (positive/negative/neutral)
2. sentiment_score: æƒ…æ„Ÿåˆ†æ•° (-1.0 åˆ° 1.0ï¼Œè´Ÿé¢ä¸ºè´Ÿï¼Œæ­£é¢ä¸ºæ­£)
3. emotion_tags: æƒ…ç»ªæ ‡ç­¾åˆ—è¡¨ (å¯é€‰: å–œæ‚¦ã€å…´å¥‹ã€æ»¡è¶³ã€æ„Ÿæ¿€ã€çˆ±ã€æ„¤æ€’ã€åŒæ¶ã€æ‚²ä¼¤ã€ææƒ§ã€å¤±æœ›ã€æƒŠè®¶ã€å›°æƒ‘ã€å¥½å¥‡ã€æœŸå¾…ã€ç„¦è™‘ã€å¹³é™ã€æ— èŠã€å†·æ¼ )

è¯·åªè¾“å‡º JSON æ ¼å¼ç»“æœï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""

# æœ‰æ•ˆæƒ…ç»ªæ ‡ç­¾
VALID_EMOTIONS = {
    "å–œæ‚¦", "å…´å¥‹", "æ»¡è¶³", "æ„Ÿæ¿€", "çˆ±",
    "æ„¤æ€’", "åŒæ¶", "æ‚²ä¼¤", "ææƒ§", "å¤±æœ›",
    "æƒŠè®¶", "å›°æƒ‘", "å¥½å¥‡", "æœŸå¾…", "ç„¦è™‘",
    "å¹³é™", "æ— èŠ", "å†·æ¼ "
}


class QwenAnalyzer:
    """Qwen2.5 æƒ…æ„Ÿåˆ†æå™¨"""

    def __init__(self, model_name: str = MODEL_NAME):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is not None:
            return

        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")

    def parse_response(self, response: str) -> Dict[str, Any]:
        """è§£ææ¨¡å‹å“åº”"""
        # å°è¯•æå– JSON
        json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)

        if json_match:
            try:
                result = json.loads(json_match.group())

                # è§„èŒƒåŒ– sentiment
                sentiment = result.get("sentiment", "neutral")
                if sentiment not in ["positive", "negative", "neutral"]:
                    sentiment = "neutral"

                # è§„èŒƒåŒ– score
                score = result.get("sentiment_score", 0.0)
                if not isinstance(score, (int, float)):
                    score = 0.0
                score = max(-1.0, min(1.0, float(score)))

                # è§„èŒƒåŒ– emotion_tags
                emotion_tags = result.get("emotion_tags", [])
                if isinstance(emotion_tags, str):
                    emotion_tags = [emotion_tags]
                elif not isinstance(emotion_tags, list):
                    emotion_tags = []
                # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾
                emotion_tags = [t for t in emotion_tags if t in VALID_EMOTIONS]
                if not emotion_tags:
                    emotion_tags = ["å¹³é™"]

                return {
                    "sentiment": sentiment,
                    "sentiment_score": score,
                    "emotion_tags": emotion_tags
                }
            except json.JSONDecodeError:
                pass

        # è§£æå¤±è´¥
        return {
            "sentiment": "neutral",
            "sentiment_score": 0.0,
            "emotion_tags": ["å¹³é™"]
        }

    def predict(self, text: str) -> Dict[str, Any]:
        """é¢„æµ‹å•æ¡æ–‡æœ¬"""
        self.load_model()

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š\n\n{text}"}
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.1,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][inputs["input_ids"].shape[1]:],
            skip_special_tokens=True
        )

        return self.parse_response(response)

    def predict_batch(self, texts: List[str], show_progress: bool = True) -> List[Dict[str, Any]]:
        """æ‰¹é‡é¢„æµ‹"""
        self.load_model()

        results = []
        iterator = tqdm(texts, desc="åˆ†æä¸­") if show_progress else texts

        for text in iterator:
            try:
                result = self.predict(text)
                results.append(result)
            except Exception as e:
                logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
                results.append({
                    "sentiment": "neutral",
                    "sentiment_score": 0.0,
                    "emotion_tags": ["å¹³é™"]
                })

        return results


def analyze_content(analyzer: QwenAnalyzer, batch_size: int = 50, dry_run: bool = False) -> Dict[str, int]:
    """åˆ†æå†…å®¹æ•°æ®"""
    from SentimentModel.database import SentimentContentRepo

    stats = {"total": 0, "success": 0, "error": 0}

    # ç»Ÿè®¡å¾…åˆ†ææ•°é‡
    unanalyzed_count = SentimentContentRepo.count_unanalyzed()
    logger.info(f"å¾…åˆ†æå†…å®¹: {unanalyzed_count} æ¡")

    if unanalyzed_count == 0:
        logger.info("æ²¡æœ‰å¾…åˆ†æçš„å†…å®¹")
        return stats

    while True:
        # è·å–ä¸€æ‰¹æ•°æ®
        data = SentimentContentRepo.get_unanalyzed(limit=batch_size)

        if not data:
            break

        stats["total"] += len(data)

        # æå–æ–‡æœ¬ (ä¼˜å…ˆä½¿ç”¨ content_cleanedï¼Œæ²¡æœ‰åˆ™ç”¨ title_cleaned)
        texts = []
        for item in data:
            text = item.get("content_cleaned") or item.get("title_cleaned") or ""
            texts.append(text[:500])  # é™åˆ¶é•¿åº¦

        # é¢„æµ‹
        try:
            predictions = analyzer.predict_batch(texts, show_progress=True)

            # ç»„è£…ç»“æœ
            results = []
            for item, pred in zip(data, predictions):
                results.append({
                    "unified_id": item["unified_id"],
                    "sentiment": pred["sentiment"],
                    "sentiment_score": pred["sentiment_score"],
                    "emotion_tags": pred["emotion_tags"]
                })

            if not dry_run:
                # æ›´æ–°æ•°æ®åº“
                updated = SentimentContentRepo.batch_update_sentiment(results)
                stats["success"] += updated
                logger.info(f"å·²æ›´æ–° {updated} æ¡å†…å®¹")
            else:
                stats["success"] += len(results)
                logger.info(f"[è¯•è¿è¡Œ] åˆ†æäº† {len(results)} æ¡å†…å®¹")

                # æ‰“å°éƒ¨åˆ†ç»“æœ
                for i, (item, pred) in enumerate(zip(data[:3], predictions[:3])):
                    text = texts[i][:50] + "..." if len(texts[i]) > 50 else texts[i]
                    print(f"  {text}")
                    print(f"    -> {pred['sentiment']} ({pred['sentiment_score']:.2f}) {pred['emotion_tags']}")

        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            stats["error"] += len(data)

    return stats


def analyze_comment(analyzer: QwenAnalyzer, batch_size: int = 50, dry_run: bool = False) -> Dict[str, int]:
    """åˆ†æè¯„è®ºæ•°æ®"""
    from SentimentModel.database import SentimentCommentRepo

    stats = {"total": 0, "success": 0, "error": 0}

    # ç»Ÿè®¡å¾…åˆ†ææ•°é‡
    unanalyzed_count = SentimentCommentRepo.count_unanalyzed()
    logger.info(f"å¾…åˆ†æè¯„è®º: {unanalyzed_count} æ¡")

    if unanalyzed_count == 0:
        logger.info("æ²¡æœ‰å¾…åˆ†æçš„è¯„è®º")
        return stats

    while True:
        data = SentimentCommentRepo.get_unanalyzed(limit=batch_size)

        if not data:
            break

        stats["total"] += len(data)

        texts = [item.get("content_cleaned", "")[:500] for item in data]

        try:
            predictions = analyzer.predict_batch(texts, show_progress=True)

            results = []
            for item, pred in zip(data, predictions):
                results.append({
                    "unified_id": item["unified_id"],
                    "sentiment": pred["sentiment"],
                    "sentiment_score": pred["sentiment_score"],
                    "emotion_tags": pred["emotion_tags"]
                })

            if not dry_run:
                updated = SentimentCommentRepo.batch_update_sentiment(results)
                stats["success"] += updated
                logger.info(f"å·²æ›´æ–° {updated} æ¡è¯„è®º")
            else:
                stats["success"] += len(results)
                logger.info(f"[è¯•è¿è¡Œ] åˆ†æäº† {len(results)} æ¡è¯„è®º")

        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            stats["error"] += len(data)

    return stats


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Qwen2.5 åˆ†ææ•°æ®åº“æ•°æ®")
    parser.add_argument("--type", choices=["all", "content", "comment"], default="all", help="åˆ†æç±»å‹")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œ (ä¸ä¿å­˜åˆ°æ•°æ®åº“)")
    parser.add_argument("--model", default=MODEL_NAME, help="æ¨¡å‹åç§°")

    args = parser.parse_args()

    print("=" * 60)
    print("Qwen2.5 æ•°æ®åº“æƒ…æ„Ÿåˆ†æ")
    print("=" * 60)
    print(f"æ¨¡å‹: {args.model}")
    print(f"ç±»å‹: {args.type}")
    print(f"æ‰¹æ¬¡: {args.batch_size}")
    print(f"è¯•è¿è¡Œ: {args.dry_run}")
    print("=" * 60)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = QwenAnalyzer(model_name=args.model)

    total_stats = {"total": 0, "success": 0, "error": 0}

    # åˆ†æå†…å®¹
    if args.type in ["all", "content"]:
        print("\nğŸ“ åˆ†æå†…å®¹...")
        stats = analyze_content(analyzer, args.batch_size, args.dry_run)
        print(f"å†…å®¹åˆ†æå®Œæˆ: æ€»æ•° {stats['total']}, æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['error']}")
        for k in total_stats:
            total_stats[k] += stats[k]

    # åˆ†æè¯„è®º
    if args.type in ["all", "comment"]:
        print("\nğŸ’¬ åˆ†æè¯„è®º...")
        stats = analyze_comment(analyzer, args.batch_size, args.dry_run)
        print(f"è¯„è®ºåˆ†æå®Œæˆ: æ€»æ•° {stats['total']}, æˆåŠŸ {stats['success']}, å¤±è´¥ {stats['error']}")
        for k in total_stats:
            total_stats[k] += stats[k]

    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆ!")
    print(f"æ€»è®¡: {total_stats['total']} æ¡")
    print(f"æˆåŠŸ: {total_stats['success']} æ¡")
    print(f"å¤±è´¥: {total_stats['error']} æ¡")
    print("=" * 60)


if __name__ == "__main__":
    main()
