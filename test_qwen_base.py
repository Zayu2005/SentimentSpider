# -*- coding: utf-8 -*-
"""
æµ‹è¯• Qwen2.5 åŸºç¡€æ¨¡å‹çš„æƒ…æ„Ÿåˆ†æèƒ½åŠ› (æ— éœ€å¾®è°ƒ)
"""

import os
from pathlib import Path

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
env_file = Path(__file__).parent / ".env"
load_dotenv(str(env_file))

# è®¾ç½® HuggingFace é•œåƒ (å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼)
if not os.getenv("HF_ENDPOINT"):
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json

# æ¨¡å‹åç§° (å¯é€‰: Qwen/Qwen2.5-0.5B-Instruct, Qwen/Qwen2.5-1.5B-Instruct, Qwen/Qwen2.5-3B-Instruct)
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œè¾“å‡ºä»¥ä¸‹ä¿¡æ¯ï¼š
1. sentiment: æƒ…æ„Ÿå€¾å‘ (positive/negative/neutral)
2. sentiment_score: æƒ…æ„Ÿåˆ†æ•° (-1.0 åˆ° 1.0ï¼Œè´Ÿé¢ä¸ºè´Ÿï¼Œæ­£é¢ä¸ºæ­£)
3. emotion_tags: æƒ…ç»ªæ ‡ç­¾åˆ—è¡¨ (å¯é€‰: å–œæ‚¦ã€å…´å¥‹ã€æ»¡è¶³ã€æ„Ÿæ¿€ã€çˆ±ã€æ„¤æ€’ã€åŒæ¶ã€æ‚²ä¼¤ã€ææƒ§ã€å¤±æœ›ã€æƒŠè®¶ã€å›°æƒ‘ã€å¥½å¥‡ã€æœŸå¾…ã€ç„¦è™‘ã€å¹³é™ã€æ— èŠã€å†·æ¼ )

è¯·åªè¾“å‡º JSON æ ¼å¼ç»“æœï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""


def load_model():
    """åŠ è½½æ¨¡å‹"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
    print("é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,  # ä½¿ç”¨ fp16 èŠ‚çœæ˜¾å­˜
        device_map="auto",
        trust_remote_code=True
    )

    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer


def predict(model, tokenizer, text):
    """é¢„æµ‹å•æ¡æ–‡æœ¬"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼š\n\n{text}"}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    return response


def main():
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model()

    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è¿™ä¸ªäº§å“çœŸçš„å¤ªå¥½ç”¨äº†ï¼å¼ºçƒˆæ¨èï¼",
        "æˆ‘å¾ˆå–œæ¬¢è¿™ä¸ªï¼Œæ„Ÿè§‰å¾ˆæ»¡æ„",
        "å¤ªæ£’äº†ï¼Œè¶…å‡ºé¢„æœŸ",
        "å¥½å¼€å¿ƒï¼Œç»ˆäºä¹°åˆ°äº†",
        "åƒåœ¾äº§å“ï¼Œåƒä¸‡åˆ«ä¹°",
        "å¤ªå·®äº†ï¼Œéå¸¸å¤±æœ›",
        "ä¸å¥½ç”¨ï¼Œé€€è´§äº†",
        "å¾ˆå¤±æœ›ï¼Œå’Œæè¿°ä¸ç¬¦",
        "è¿˜è¡Œå§ï¼Œä¸€èˆ¬èˆ¬",
        "æ²¡ä»€ä¹ˆç‰¹åˆ«çš„æ„Ÿè§‰",
    ]

    print("\n" + "=" * 70)
    print("Qwen2.5 æƒ…æ„Ÿåˆ†ææµ‹è¯•")
    print("=" * 70)

    for text in test_texts:
        print(f"\nğŸ“ æ–‡æœ¬: {text}")
        response = predict(model, tokenizer, text)
        print(f"ğŸ¤– è¾“å‡º: {response}")

        # å°è¯•è§£æ JSON
        try:
            # æå– JSON éƒ¨åˆ†
            import re
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                print(f"   âœ… æƒ…æ„Ÿ: {result.get('sentiment', 'N/A')}")
                print(f"   âœ… åˆ†æ•°: {result.get('sentiment_score', 'N/A')}")
                print(f"   âœ… æƒ…ç»ª: {result.get('emotion_tags', 'N/A')}")
        except:
            print("   âš ï¸ JSON è§£æå¤±è´¥")

        print("-" * 50)


if __name__ == "__main__":
    main()
